<div align="center">
<img src="https://github.com/skswar/Data_Engineering_Pipelines/blob/main/img/banner.png" alt="Intro Logo" width="75%" height="30%"/></div>
<p align="right">
<a rel="license" href="http://creativecommons.org/licenses/by/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by/4.0/80x15.png" width="5%"/></a><br/><ruby><rt>This image is licensed by Sayan Swar under a <a rel="license" href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution 4.0 International License</a>.</rt></ruby>
</p>
</div>

<h3 align="center">How to build Efficient Data Enginnerg Pipeline at No Additional Software Overhead Cost?</h4>

<hr>

## Table of contents
* [Introduction](#introduction)
* [Methodology](#methodology)

## Introduction
As the global business landscape is increasingly transitioning towards data-driven decision-making and artificial intelligence, the importance of gathering and structuring data in an organized manner is crucial for all organizations. With all the technologies available at our disposal building, data pipleines and data storage has become very easy and fast. But it is not always economically viable, especially for growing business and startups. Often, immediate investment in high-end software or cloud solutions might not be feasible. Or say, in some cases the workload might not warrant the need for large-scale software solutions and can be effectively handled with in house, open source tools. 

In this project I illustrate the process of implementing automated data pipleines with open source tools. I aim to demonstrate how simple solutions and proactive initiatives can empower us to commence data gathering from day one and offer valuable insights for strategic decision-making.
